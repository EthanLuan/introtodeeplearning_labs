{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part1_tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "57knM8jrYZ2t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/aamini/introtodeeplearning_labs/blob/master/lab1/Part1_tensorflow.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/colab.png?v2.0\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/aamini/introtodeeplearning_labs/blob/master/lab1/Part1_tensorflow.ipynb\">\n",
        "        <img src=\"http://introtodeeplearning.com/images/colab/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Lab 1: Intro to TensorFlow and Music Generation with RNNs\n",
        "# Part 1: Intro to TensorFlow"
      ]
    },
    {
      "metadata": {
        "id": "OhuYRQfjYZ2v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0.1 Install TensorFlow\n",
        "\n",
        "TensorFlow is a software library extensively used in machine learning. Here we'll learn how computations are represented and how to define a simple neural network in TensorFlow.\n",
        "\n",
        "Let's install TensorFlow and a couple of dependencies: \n"
      ]
    },
    {
      "metadata": {
        "id": "LkaimNJfYZ2w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3oWpEMtmYZ3I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll then check to make sure things installed properly:"
      ]
    },
    {
      "metadata": {
        "id": "zLLaY8hvdbvQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "is_correct_tf_version = '1.13.' in tf.__version__\n",
        "assert is_correct_tf_version, \"Wrong tensorflow version {} installed\".format(tf.__version__)\n",
        "\n",
        "is_eager_enabled = tf.executing_eagerly()\n",
        "assert is_eager_enabled,      \"Tensorflow eager mode is not enabled\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDJGsR2NoYtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TensorFlow is set to release the next major version of TensorFlow, [TensorFlow 2.0](https://www.tensorflow.org/community/roadmap#tensorflow_20_is_coming), very soon. In this set of labs we'll be working in TensorFlow 1.12.0. The 6.S191 team is **Eager** to show you this version, as it features a (relatively) new imperative programming style called Eager execution. Under Eager execution, TensorFlow operations execute immediately as they're called from Python (which wasn't always the case!). This allows for fast debugging and a more intuitive way to get started with TensorFlow.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iD3VO-LZYZ2z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1 The computation graph\n",
        "\n",
        "TensorFlow is called TensorFlow because it handles the flow (node/mathematical operation) of Tensors (data), which you can think of as multidimensional arrays. In TensorFlow, computations can be thought of as graphs. First, we'll explore defining a computational graph with Tensors and mathematical operations before diving in to how we can build deep learning models in TensorFlow. \n",
        "\n",
        "Let's look at a simple example, and define this computation using TensorFlow:\n",
        "\n",
        "![alt text](img/add-graph.png \"Computation Graph\")\n",
        "\n",
        "<!-- Keras is a high-level API to build and train deep learning models. It's used for fast prototyping, advanced research, and production, with three key advantages:\n",
        "\n",
        "User friendly\n",
        "Keras has a simple, consistent interface optimized for common use cases. It provides clear and actionable feedback for user errors.\n",
        "Modular and composable\n",
        "Keras models are made by connecting configurable building blocks together, with few restrictions.\n",
        "Easy to extend\n",
        "Write custom building blocks to express new ideas for research. Create new layers, loss functions, and develop state-of-the-art models. -->\n",
        "<!-- \n",
        "TensorFlow programs are usually structured into a phase that assembles a graph, and a phase that uses a session to execute operations in the graph. In TensorFlow we define the computational graph with Tensors and mathematical operations to create a system for machine learning and deep learning.\n",
        "\n",
        "We can think of a computation graph as a series of math operations that occur in some order.  -->\n"
      ]
    },
    {
      "metadata": {
        "id": "X_YJrZsxYZ2z",
        "colab_type": "code",
        "outputId": "caab4f88-7def-4612-e404-eeacaff4c515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Create the nodes in the graph, and initialize values\n",
        "a = tf.constant(15, name=\"a\")\n",
        "b = tf.constant(61, name=\"b\")\n",
        "\n",
        "# Add them!\n",
        "c = tf.add(a,b, name=\"c\")\n",
        "print(c)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(76, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mjYCF0EdYZ22",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how we've created a computation graph consisting of TensorFlow operations, and how  the output is a Tensor with value 76 -- we've just created a computation graph consisting of operations, and it's executed them and given us back the result. That's because of Eager!"
      ]
    },
    {
      "metadata": {
        "id": "Mbfv_QOiYZ23",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building a computation graph\n",
        "\n",
        "Now let's consider a slightly more complicated computation graph:\n",
        "![alt text](img/computation-graph.png \"Computation Graph\")\n",
        "\n",
        "This graph takes two inputs, `a, b`, and computes an output `e`. Each node in the graph is an operation that takes some input, does some computation, and passes its output to another node.\n",
        "\n",
        "Let's define a simple function in TensorFlow to construct this computation graph:"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "PJnfzpWyYZ23",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Construct a simple computation graph\n",
        "def graph(a,b):\n",
        "  '''TODO: Define the operation for c, d, e (use tf.add, tf.subtract, tf.multiply).'''\n",
        "  c = tf.add(a,b,name='c')\n",
        "  d = tf.subtract(b,1,name='d')\n",
        "  e = tf.multiply(c,d,name='e')\n",
        "  return e"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AwrRfDMS2-oy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can call this function to execute the computation graph given some inputs `a,b`:"
      ]
    },
    {
      "metadata": {
        "id": "pnwsf8w2uF7p",
        "colab_type": "code",
        "outputId": "af837526-cfd1-4c02-c0c9-b4734d707f44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Consider example values for a,b\n",
        "a, b = 1.5, 2.5\n",
        "# Execute the computation\n",
        "e_out = graph(a,b)\n",
        "print (e_out)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6HqgUIUhYZ29",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, notice how our output is a Tensor with value defined by the output of the computation (thanks to Eager!)."
      ]
    },
    {
      "metadata": {
        "id": "1h4o9Bb0YZ29",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2 Neural networks in TensorFlow\n",
        "We can also define neural networks in TensorFlow, and it's often helpful to think about this using the idea of computation graphs. TensorFlow uses a high-level API called [Keras](https://www.tensorflow.org/guide/keras) that provides a powerful, intuitive framework for building and training deep learning models. In the 6.S191 labs we'll be using the Keras API to build and train our models.\n",
        "\n",
        "Let's consider this example of a very simple neural network of just one dense layer:\n",
        "\n",
        "![alt text](img/computation-graph-2.png \"Computation Graph\")\n",
        "\n",
        "This graph takes an input `x` and computes an output `out`. It does so how we learned in lecture today: `out = sigmoid(W*x+b)`.\n",
        "\n",
        "First, let's define this computation graph in TensorFlow via a simple function, as we did before:"
      ]
    },
    {
      "metadata": {
        "id": "ToJIeFqNcLAR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# n_in: number of inputs\n",
        "# n_out: number of outputs\n",
        "def our_dense_layer(x, n_in, n_out):\n",
        "  # Define and initialize parameters, a weight matrix W and biases b\n",
        "  W = tf.Variable(tf.ones((n_in, n_out)))\n",
        "  b = tf.Variable(tf.zeros((1, n_out)))\n",
        "  \n",
        "  '''TODO: define the operation for z (hint: use tf.matmul)'''\n",
        "  z = tf.matmul(x,W) + b\n",
        "  \n",
        "  '''TODO: define the operation for out (hint: use tf.sigmoid)'''\n",
        "  out = tf.sigmoid(z)\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OgSBEuEtwb2e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As before, we can define an example input, feed it into `our_dense_layer` function, and immediately execute:"
      ]
    },
    {
      "metadata": {
        "id": "PSI3I0CFcxnv",
        "colab_type": "code",
        "outputId": "ddfa0bf6-0695-464c-d11a-556fa43065a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "'''TODO: define an example input x_input'''\n",
        "x_input = tf.constant([[1.,2.,5.],[3.,4.,6.]])\n",
        "'''TODO: call `our_dense_layer` to get the output of the network and print the result!'''\n",
        "# our_dense_layer() \n",
        "print (our_dense_layer(x_input,x_input.shape[1],3))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "tf.Tensor(\n",
            "[[0.99966455 0.99966455 0.99966455]\n",
            " [0.99999774 0.99999774 0.99999774]], shape=(2, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jt1FgM7qYZ3D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, instead of explicitly defining a simple function, we'll use the Keras API to define our neural network. This will be especially important as we move on to more complicated network architectures. \n",
        "\n",
        "Specifically, for this network we'll use the Keras [`Sequential`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential) model from the `tf.keras` API to define our network. The `tf.keras.Sequential` model lets us conveniently define a linear stack of network layers. We'll use [`tf.keras.layers.Dense` ](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) to define our single fully connected network layer. "
      ]
    },
    {
      "metadata": {
        "id": "7WXTpmoL6TDz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import relevant packages\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Define the number of inputs and outputs\n",
        "n_input_nodes = 2\n",
        "n_output_nodes = 3\n",
        "\n",
        "# First define the model \n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "'''TODO: Define a dense (fully connected) layer to compute z'''\n",
        "# Remember: dense layers are defined by the parameters W and b!\n",
        "# You can read more about the initialization of W and b in the TF documentation :) \n",
        "dense_layer = Dense(n_output_nodes, input_shape=(n_input_nodes,)) # TODO\n",
        "\n",
        "# Add the dense layer to the model\n",
        "model.add(dense_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HDGcwYfUyR-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's it! We've defined our model. Now, we can test it out using an example input:"
      ]
    },
    {
      "metadata": {
        "id": "sg23OczByRDb",
        "colab_type": "code",
        "outputId": "56acc32e-7530-46fd-9c0b-b4441630a43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        }
      },
      "cell_type": "code",
      "source": [
        "# Test model with example input\n",
        "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
        "\n",
        "'''TODO: feed input into the model and predict the output!'''\n",
        "# print # TODO\n",
        "print(model.fit(x_input))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b866a71dfd90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m'''TODO: feed input into the model and predict the output!'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print # TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m   \u001b[0;31m# Get step function and loop type.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0muse_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(model, mode)\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_distributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_execution_function\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   1981\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_execution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1983\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1984\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_fit_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     ]\n\u001b[1;32m   1925\u001b[0m     self._make_train_function_helper(\n\u001b[0;32m-> 1926\u001b[0;31m         '_fit_function', [self.total_loss] + metrics_tensors)\n\u001b[0m\u001b[1;32m   1927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_test_function_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_updates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'total_loss'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "dQwDhKn8kbO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.3 Automatic differentiation\n",
        "\n",
        "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
        "is one of the most important parts of TensorFlow and is the backbone of training with \n",
        "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation). During Eager execution, use `tf.GradientTape` to trace operations for computing gradients later. \n",
        "\n",
        "All forward-pass operations get recorded to a \"tape\"; then, to compute the gradient, the tape is played backwards and then discarded. A particular `tf.GradientTape` can only\n",
        "compute one gradient; subsequent calls throw a runtime error.\n",
        "\n",
        "Let's take a look at a simple example! We can use automatic differentiation and stochastic gradient descent (SGD) to find the minimum of $y=(x-1)^2$. While we can clearly solve this problem analytically ($x_{min}=1$), solving this simple example sets us up nicely for future labs where we use gradient descent to optimize entire neural network losses. "
      ]
    },
    {
      "metadata": {
        "attributes": {
          "classes": [
            "py"
          ],
          "id": ""
        },
        "colab_type": "code",
        "id": "7g1yWiSXqEf-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.Variable([tf.random.normal([1])])\n",
        "print \"Initializing x={}\".format(x.numpy())\n",
        "learning_rate = 1e-2\n",
        "history = []\n",
        "\n",
        "for i in range(500):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = (x - 1)**2 # record the forward pass on the tape\n",
        "\n",
        "  grad = tape.gradient(y, x) # compute the gradient of y with respect to x\n",
        "  new_x = x - learning_rate*grad # sgd update\n",
        "  x.assign(new_x) # update the value of x\n",
        "  history.append(x.numpy()[0])\n",
        "\n",
        "plt.plot(history)\n",
        "plt.plot([0, 500],[1,1])\n",
        "plt.legend(('Predicted', 'True'))\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('x value')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "UxBEH9__YZ3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.4 Control flow\n",
        "\n",
        "As you've seen, TensorFlow now an imperative programming style, and that's all because of Eager. \n",
        "\n",
        "As another example of the power of Eager, let's take a look at how we can build a dynamic model that uses Python flow control. Here's an example of the [Collatz conjecture](https://en.wikipedia.org/wiki/Collatz_conjecture) using TensorFlow’s arithmetic operations. Such dynamic behavior is not possible in past versions of TensorFlow (up to v1.4)!"
      ]
    },
    {
      "metadata": {
        "id": "LCfX4kfRYZ3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = tf.constant(12)\n",
        "counter = 0\n",
        "while not tf.equal(a, 1):\n",
        "  if tf.equal(a % 2, 0):\n",
        "    a = a / 2\n",
        "  else:\n",
        "    a = 3 * a + 1\n",
        "  print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}